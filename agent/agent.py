from typing import Dict, List, Type, Any, Callable
from langchain.output_parsers import PydanticOutputParser
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel

from data_processor import DataProcessor


# Evaluation Agent
class Agent:
    def __init__(self, llm_name: str, data_processor: DataProcessor) -> None:
        """
        Initializes the Agent with a language model and a data processor.

        Args:
            llm_name (str): The name of the language model to be used.
            data_processor (DataProcessor): An instance of the DataProcessor class to handle data loading and processing.
        """
        self.model = Ollama(model=llm_name)
        self.data_processor = data_processor

    def llm_recommendation(
        self,
        process_user_settings: Callable[[Dict[str, Any]], str],
        output_model: Type[BaseModel],
        template: str) -> Any:
        """
        Generalized method for generating recommendations based on user input.

        Args:
            process_user_settings (Callable[[Dict[str, Any]], str]): A function that processes user settings into a prompt.
            output_model (Type[BaseModel]): The Pydantic model to parse the output.
            template (str): The prompt template to be used for generating the output.

        Returns:
            Any: The parsed result based on the provided output model.
        """
        user_prompt = self.data_processor.convert_user(process_user_settings)
        parser = PydanticOutputParser(pydantic_object=output_model)
        prompt = PromptTemplate(
            template=template,
            input_variables=["user_input"],
            partial_variables={"format_instructions": parser.get_format_instructions()})
        chain = prompt | self.model | parser
        res = chain.invoke({"user_input": user_prompt})
        return res

    def evaluate(
        self,
        process_evaluation_data: Callable[[Dict[str, Any], int], List[Any]],
        evaluation_model: Type[BaseModel],
        template: str,
        topn: int) -> Any:
        """
        Generalized method for evaluating the output generated by the LLM.

        Args:
            process_evaluation_data (Callable[[Dict[str, Any], int], List[Any]]): A function that processes the data to be evaluated.
            evaluation_model (Type[BaseModel]): The Pydantic model to parse the evaluation output.
            template (str): The prompt template to be used for the evaluation.
            topn (int): The number of top items to evaluate.

        Returns:
            Any: The evaluation score or result based on the provided evaluation model.
        """
        data_to_evaluate = self.data_processor.convert_response(process_evaluation_data, topn)
        parser = PydanticOutputParser(pydantic_object=evaluation_model)
        prompt = PromptTemplate(
            template=template,
            input_variables=["user_input", "data_to_evaluate"],
            partial_variables={"format_instructions": parser.get_format_instructions()})
        chain = prompt | self.model | parser
        res = chain.invoke({
            "user_input": self.data_processor.convert_user(lambda x: x),
            "data_to_evaluate": data_to_evaluate})
        return res
